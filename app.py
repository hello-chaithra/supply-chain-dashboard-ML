# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G_Z1RvfFOkMRp1F188-5ZZgVCqrqjPW5

**Phase 1: Data Foundation (SQL & Pandas)**

Goal: SQL schema design, ETL basics, Pandas cleaning patterns

1.   Get real data, clean it, store it properly.
2.   Load into a SQLite database.
3.   SQL queries for key metrics: inventory turnover, stockout rates.
4.   Pull into Pandas for transformation.
"""

# Install
import pandas as pd
import numpy as np

# Download the DataCo Supply Chain dataset (10k+ records)
url = "https://raw.githubusercontent.com/dsrscientist/dataset1/master/e-commerce.csv"

#Quick synthetic dataset to start immediately
np.random.seed(42)
n = 10000

df = pd.DataFrame({
    'order_id': range(1, n+1),
    'date': pd.date_range(start='2022-01-01', periods=n, freq='D').strftime('%Y-%m-%d'),
    'product_category': np.random.choice(['Electronics', 'Clothing', 'Food', 'Tools', 'Medical'], n),
    'warehouse_region': np.random.choice(['North', 'South', 'East', 'West'], n),
    'units_ordered': np.random.randint(1, 500, n),
    'units_in_stock': np.random.randint(0, 1000, n),
    'lead_time_days': np.random.randint(1, 30, n),
    'shipping_cost': np.round(np.random.uniform(5, 500, n), 2),
    'on_time_delivery': np.random.choice([0, 1], n, p=[0.2, 0.8])
})

df['date'] = pd.to_datetime(df['date'])

print(f"Dataset shape: {df.shape}")
print(df.head())
print("\nData types:")
print(df.dtypes)

import sqlite3
from sqlalchemy import create_engine

#Create SQLite database
engine = create_engine('sqlite:///supply_chain.db')

# Load dataframe into SQL as a table called 'orders'
df.to_sql('orders', con=engine, if_exists='replace', index=False)
print("✅ Data loaded into SQLite database")

# Query it with pure SQL (sqlite3: see raw SQL syntax clearly)

conn = sqlite3.connect('supply_chain.db')

# Query 1: Total units ordered per product category
query1 = """
SELECT
    product_category,
    COUNT(*) AS total_orders,
    SUM(units_ordered) AS total_units,
    ROUND(AVG(shipping_cost), 2) AS avg_shipping_cost,
    ROUND(AVG(lead_time_days), 1) AS avg_lead_time
FROM orders
GROUP BY product_category
ORDER BY total_units DESC;
"""

# Query 2: On-time delivery rate by warehouse region
query2 = """
SELECT
    warehouse_region,
    COUNT(*) AS total_orders,
    SUM(on_time_delivery) AS on_time_count,
    ROUND(100.0 * SUM(on_time_delivery) / COUNT(*), 1) AS on_time_rate_pct
FROM orders
GROUP BY warehouse_region
ORDER BY on_time_rate_pct DESC;
"""

# Query 3: Stockout risk — orders where units ordered > units in stock
query3 = """
SELECT
    product_category,
    warehouse_region,
    COUNT(*) AS stockout_risk_orders,
    ROUND(AVG(units_ordered - units_in_stock), 0) AS avg_shortfall
FROM orders
WHERE units_ordered > units_in_stock
GROUP BY product_category, warehouse_region
ORDER BY stockout_risk_orders DESC
LIMIT 10;
"""

print("\n Query 1: Category Performance")
print(pd.read_sql(query1, conn).to_string(index=False))

print("\n Query 2: On-Time Delivery by Region")
print(pd.read_sql(query2, conn).to_string(index=False))

print("\n Query 3: Top Stockout Risks")
print(pd.read_sql(query3, conn).to_string(index=False))

conn.close()

"""**Phase 2: Exploratory Analysis (Pandas & NumPy)**

Goal: GroupBy aggregations, time-series indexing, NumPy vectorized ops

1.   Demand distribution analysis
2.   Seasonality detection
3.   Warehouse/supplier performance metrics
4.   Generate summary stats on 10k+ records
"""

import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Chart 1: Monthly demand trend
monthly = df.groupby(df['date'].dt.to_period('M'))['units_ordered'].sum().reset_index()
monthly['date'] = monthly['date'].astype(str)

fig1 = px.line(monthly, x='date', y='units_ordered',
               title=' Monthly Demand Trend (2022–2049)',
               labels={'units_ordered': 'Total Units Ordered', 'date': 'Month'},
               template='plotly_dark')
fig1.update_traces(line_color='#00d4ff', line_width=2)
fig1.show()

# Chart 2: On-time delivery rate by region (bar)
conn = sqlite3.connect('supply_chain.db')
delivery_df = pd.read_sql("""
    SELECT warehouse_region,
           ROUND(100.0 * SUM(on_time_delivery) / COUNT(*), 1) AS on_time_rate
    FROM orders GROUP BY warehouse_region
""", conn)
conn.close()

fig2 = px.bar(delivery_df, x='warehouse_region', y='on_time_rate',
              title=' On-Time Delivery Rate by Region',
              color='on_time_rate', color_continuous_scale='RdYlGn',
              range_y=[70, 85],
              labels={'on_time_rate': 'On-Time Rate (%)', 'warehouse_region': 'Region'},
              template='plotly_dark')
fig2.show()

# Chart 3: Stockout risk heatmap
conn = sqlite3.connect('supply_chain.db')
stockout_df = pd.read_sql("""
    SELECT product_category, warehouse_region, COUNT(*) AS stockout_orders
    FROM orders WHERE units_ordered > units_in_stock
    GROUP BY product_category, warehouse_region
""", conn)
conn.close()

pivot = stockout_df.pivot(index='product_category',
                           columns='warehouse_region',
                           values='stockout_orders')

fig3 = px.imshow(pivot,
                 title='Stockout Risk Heatmap (Orders at Risk)',
                 color_continuous_scale='YlOrRd',
                 template='plotly_dark',
                 text_auto=True)
fig3.show()

# Chart 4: Shipping cost distribution by category
fig4 = px.box(df, x='product_category', y='shipping_cost',
              title='Shipping Cost Distribution by Category',
              color='product_category',
              template='plotly_dark')
fig4.show()

"""**Phase 3: ML Forecasting (Scikit-learn)**

Goal: Feature engineering for time-series, train/test splits, model evaluation

1.   Demand forecasting with Linear Regression as baseline.
2.   Upgrade to Random Forest or XGBoost for trend prediction.
3.   Evaluate with MAE/RMSE.

"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

# Feature Engineering
df_ml = df.copy()
df_ml['month']      = df_ml['date'].dt.month
df_ml['dayofweek']  = df_ml['date'].dt.dayofweek
df_ml['quarter']    = df_ml['date'].dt.quarter
df_ml['dayofyear']  = df_ml['date'].dt.dayofyear

# Encodng categorical columns as numbers
df_ml['category_code'] = df_ml['product_category'].astype('category').cat.codes
df_ml['region_code']   = df_ml['warehouse_region'].astype('category').cat.codes

# Define features (X) and target (y)
features = ['month', 'dayofweek', 'quarter', 'dayofyear',
            'category_code', 'region_code',
            'lead_time_days', 'units_in_stock', 'shipping_cost']

X = df_ml[features]
y = df_ml['units_ordered']

# Train/test split — 80% train, 20% test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
print(f"Training on {len(X_train):,} rows, testing on {len(X_test):,} rows")

# Model 1: Linear Regression (baseline)
lr = LinearRegression()
lr.fit(X_train, y_train)
lr_preds = lr.predict(X_test)

lr_mae  = mean_absolute_error(y_test, lr_preds)
lr_rmse = np.sqrt(mean_squared_error(y_test, lr_preds))

# Model 2: Random Forest (model)
rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
rf.fit(X_train, y_train)
rf_preds = rf.predict(X_test)

rf_mae  = mean_absolute_error(y_test, rf_preds)
rf_rmse = np.sqrt(mean_squared_error(y_test, rf_preds))

# Results comparison
print("\n Model Performance Comparison")
print(f"{'Metric':<10} {'Linear Regression':>20} {'Random Forest':>20}")
print("-" * 52)
print(f"{'MAE':<10} {lr_mae:>20.1f} {rf_mae:>20.1f}")
print(f"{'RMSE':<10} {lr_rmse:>20.1f} {rf_rmse:>20.1f}")

improvement = round((1 - rf_mae / lr_mae) * 100, 1)
print(f"\n Random Forest is {improvement}% better on MAE")

# Feature Importance Chart
importance_df = pd.DataFrame({
    'feature': features,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=True)

fig = px.bar(importance_df, x='importance', y='feature',
             orientation='h',
             title=' What Drives Demand? (Random Forest Feature Importance)',
             template='plotly_dark',
             color='importance',
             color_continuous_scale='Blues')
fig.show()

